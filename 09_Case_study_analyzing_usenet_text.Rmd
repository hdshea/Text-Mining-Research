---
title: "09 Case study: analyzing usenet text"
author: "H. David Shea"
date: "`r format(Sys.time(), '%d %b %Y')`"
output:
  html_document:
    fig.align: center
    fig_caption: yes
    fig_height: 5
    fig_width: 9
    highlight: tango
    theme: united
    #toc: yes
  pdf_document:
    toc: yes
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(knitr)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
library(topicmodels)

knitr::opts_chunk$set(
    comment = "#>", 
    collapse = TRUE, 
    message = FALSE,
    fig.width = 8,
    fig.asp = ((1 + sqrt(5)) / 2) - 1, # the golden ratio - technically, the b proportion of a+b when a is 1
    out.width = "70%",
    fig.align = "center"
)
```

The data set used in the folowing analyses is a set of 20,000 messages sent to 20 Usenet bulletin boards in 1993.  This data set is publicly available at [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/).

##  Pre-processing

```{r c9.1a, include=TRUE}
# processing these data files takes a long time - saved to .rds file to speed up processing
if (file.exists("data/raw_text.rda")) {
    load("data/raw_text.rda")
} else {
    training_folder <- "data/20news-bydate/20news-bydate-train/"
    
    # Define a function to read all files from a folder into a data frame
    read_folder <- function(infolder) {
        tibble(file = dir(infolder, full.names = TRUE)) %>%
            mutate(text = map(file, read_lines)) %>%
            transmute(id = basename(file), text) %>%
            unnest(text)
    }
    
    # Use unnest() and map() to apply read_folder to each subfolder
    raw_text <-
        tibble(folder = dir(training_folder, full.names = TRUE)) %>%
        mutate(folder_out = map(folder, read_folder)) %>%
        unnest(cols = c(folder_out)) %>%
        transmute(newsgroup = basename(folder), id, text)
}

raw_text %>% 
    slice_head(n = 10) %>% 
    kable(caption = "Example text from `20news-bydate` data set.")
```

```{r c9.1b, include=TRUE, fig.cap="Number of messages from each newsgroup"}
raw_text %>%
    group_by(newsgroup) %>%
    summarize(messages = n_distinct(id)) %>%
    ggplot(aes(messages, newsgroup)) +
    geom_col() +
    labs(y = NULL)
```

### Pre-processing text

Clean up the text with the following prep chunks ending in unnesting the tokens.

```{r c9.1.1a, include=TRUE}
# must occur after the first occurrence of an empty line,
# and before the first occurrence of a line starting with --
cleaned_text <- raw_text %>%
    group_by(newsgroup, id) %>%
    filter(cumsum(text == "") > 0,
           cumsum(str_detect(text, "^--")) == 0) %>%
    ungroup()

# remove nested text of quotes from messages and two messages with a lot of non-text content
cleaned_text <- cleaned_text %>%
    filter(
        str_detect(text, "^[^>]+[A-Za-z\\d]") | text == "",
        !str_detect(text, "writes(:|\\.\\.\\.)$"),
        !str_detect(text, "^In article <"),
        !id %in% c(9704, 9985)
    )

usenet_words <- cleaned_text %>%
    unnest_tokens(word, text) %>%
    filter(str_detect(word, "[a-z']$"),
           !word %in% stop_words$word)
```

##  Words in newsgroups

```{r c9.2, include=TRUE}
usenet_words %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 10, with_ties = FALSE) %>%
    kable(caption = "Highest frequency words in `20news-bydate` data set.")

words_by_newsgroup <- usenet_words %>%
    count(newsgroup, word, sort = TRUE) %>%
    ungroup()

words_by_newsgroup %>%
    slice_max(n, n = 10, with_ties = FALSE) %>%
    kable(caption = "Highest frequency words by newsgroup in `20news-bydate` data set.")
```

### Finding tf-idf within newsgroups

Newsgroups should differ in terms of topic and content. As such, the frequency of words should differ between them as well.

```{r c9.2.1a, include=TRUE}
tf_idf <- words_by_newsgroup %>%
    bind_tf_idf(word, newsgroup, n) %>%
    arrange(desc(tf_idf))

tf_idf %>% 
    slice_max(tf_idf, n = 10, with_ties = FALSE) %>% 
    kable(caption = "Highest tf-idf values words by newsgroup in `20news-bydate` data set.")
```

Looking just at the `sci.` newsgroups.

```{r c9.2.1b, include=TRUE, fig.cap="Terms with the highest tf-idf within each of the science-related newsgroups"}
tf_idf %>%
    filter(str_detect(newsgroup, "^sci\\.")) %>%
    group_by(newsgroup) %>%
    slice_max(tf_idf, n = 12) %>%
    ungroup() %>%
    mutate(word = reorder(word, tf_idf)) %>%
    ggplot(aes(tf_idf, word, fill = newsgroup)) +
    geom_col(show.legend = FALSE) +
    facet_wrap( ~ newsgroup, scales = "free") +
    labs(x = "tf-idf", y = NULL)
```

```{r c9.2.1c, include=TRUE}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, word, n, sort = TRUE)

newsgroup_cors %>% 
    slice_max(correlation, n = 10, with_ties = FALSE) %>% 
    kable(caption = "Newsgroups with highest pairwise correlation of word frequencies within each newsgroup")
```

```{r c9.2.1d, include=TRUE, fig.cap="Network of Usenet groups based on the correlation of word counts between them (correlation > 0.4)"}
set.seed(2017)

newsgroup_cors %>%
    filter(correlation > .4) %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(alpha = correlation, width = correlation)) +
    geom_node_point(size = 6, color = "lightblue") +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
```

"It looks like there were four main clusters of newsgroups: computers/electronics, politics/religion, motor vehicles, and sports."

### Topic modeling

Using the four `sci.` newsgroups, use LDA to fit a topic model.

```{r c9.2.2a, include=TRUE, fig.cap="Top words from each topic fit by LDA on the science-related newsgroups"}
# include only words that occur at least 50 times
word_sci_newsgroups <- usenet_words %>%
    filter(str_detect(newsgroup, "^sci")) %>%
    group_by(word) %>%
    mutate(word_total = n()) %>%
    ungroup() %>%
    filter(word_total > 50)

# convert into a document-term matrix
# with document names such as sci.crypt_14147
sci_dtm <- word_sci_newsgroups %>%
    unite(document, newsgroup, id) %>%
    count(document, word) %>%
    cast_dtm(document, word, n)

sci_lda <- LDA(sci_dtm, k = 4, control = list(seed = 2016))

sci_lda %>%
    tidy() %>%
    group_by(topic) %>%
    slice_max(beta, n = 8) %>%
    ungroup() %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap( ~ topic, scales = "free") +
    scale_y_reordered() +
    theme_light()
```

```{r c9.2.2b, include=TRUE, fig.cap="Distribution of gamma for each topic within each Usenet newsgroup"}
sci_lda %>%
    tidy(matrix = "gamma") %>%
    separate(document, c("newsgroup", "id"), sep = "_") %>%
    mutate(newsgroup = reorder(newsgroup, gamma * topic)) %>%
    ggplot(aes(factor(topic), gamma)) +
    geom_boxplot() +
    facet_wrap( ~ newsgroup) +
    labs(x = "Topic",
         y = "# of messages where this was the highest % topic") +
    theme_light()
```

These two graphics show that:

* Topic 1 lines up with the sci.space newsgroup
* Topic 2 lines up with the sci.crypt newsgroup
* Topic 3 lines up with the sci.med newsgroup
* Topic 4 lines up with the sci.electronics newsgroup 

##  Sentiment analysis

Using the `AFINN` sentiment lexicon to analyze how often positive and negative words occur by newsgroup.

```{r c9.3, include=TRUE, fig.cap="Average AFINN value for posts within each newsgroup"}
newsgroup_sentiments <- words_by_newsgroup %>%
    inner_join(get_sentiments("afinn"), by = "word") %>%
    group_by(newsgroup) %>%
    summarize(value = sum(value * n) / sum(n))

newsgroup_sentiments %>%
    mutate(newsgroup = reorder(newsgroup, value)) %>%
    ggplot(aes(value, newsgroup, fill = value > 0)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Average sentiment value", y = NULL)
```

Politics - negative.  People selling things - positive.  Makes sense.

### Sentiment analysis by word

Examining the total positive and negative contributions of each word.

```{r c9.3.1a, include=TRUE, fig.cap="Words with the greatest contributions to positive/negative sentiment values in the Usenet text"}
contributions <- usenet_words %>%
    inner_join(get_sentiments("afinn"), by = "word") %>%
    group_by(word) %>%
    summarize(occurences = n(),
              contribution = sum(value))

contributions %>% 
    arrange(desc(abs(contribution))) %>% 
    slice_max(abs(contribution), n = 10, with_ties = FALSE) %>% 
    kable(caption = "Words with the highest contribution to sentiment scoring")

contributions %>%
    slice_max(abs(contribution), n = 25) %>%
    mutate(word = reorder(word, contribution)) %>%
    ggplot(aes(contribution, word, fill = contribution > 0)) +
    geom_col(show.legend = FALSE) +
    labs(y = NULL) +
    theme_light()
```

This approach still has the drawback that words like 'true' and 'bad' (top positive and top negative words) could be incorrectly categorized if they were represented in the text as 'not true' and 'not bad'.

```{r c9.3.1b, include=TRUE, fig.cap = "Words that contributed the most to sentiment scores within each of six newsgroups"}
top_sentiment_words <- words_by_newsgroup %>%
    inner_join(get_sentiments("afinn"), by = "word") %>%
    mutate(contribution = value * n / sum(n))

top_sentiment_words %>%
    slice_head(n = 10) %>%
    kable(caption = "Contribution of each word to each newsgroup’s sentiment score")

top_sentiment_words %>%
    filter(str_detect(newsgroup, "^(talk|alt|misc)")) %>%
    group_by(newsgroup) %>%
    slice_max(abs(contribution), n = 12) %>%
    ungroup() %>%
    mutate(
        newsgroup = reorder(newsgroup, contribution),
        word = reorder_within(word, contribution, newsgroup)
    ) %>%
    ggplot(aes(contribution, word, fill = contribution > 0)) +
    geom_col(show.legend = FALSE) +
    scale_y_reordered() +
    facet_wrap( ~ newsgroup, scales = "free") +
    labs(x = "Sentiment value * # of occurrences", y = NULL) +
    theme_light()
```

Another drawback is shown here where 'god' and 'jesus' show up as high positive contribution in alt.atheism and talk.religion.misc but likely have different true sentiment between those groups.  And similarly, 'gun' is the highest negative sentiment contributor in the talk.politics.guns newsgroup where the term is very likely used in a positive sentiment.

### Sentiment analysis by message

Examining the sentiment by individual message.

```{r c9.3.2a, include=TRUE}
sentiment_messages <- usenet_words %>%
    inner_join(get_sentiments("afinn"), by = "word") %>%
    group_by(newsgroup, id) %>%
    summarize(sentiment = mean(value),
              words = n()) %>%
    ungroup() %>%
    filter(words >= 5)

sentiment_messages %>%
    arrange(desc(sentiment)) %>%
    slice_min(sentiment, n = 10, with_ties = FALSE) %>%
    kable(caption = "Most positive messages")

# function to print an individual message
print_message <- function(group, message_id) {
    result <- cleaned_text %>%
        filter(newsgroup == group, id == message_id, text != "")
    
    cat(result$text, sep = "\n")
}
```

Winner! Winner! Chicken dinner!

```{r c9.3.2b, include=TRUE}
# Most positive message
print_message("rec.sport.hockey", 53560)

sentiment_messages %>%
  arrange(sentiment) %>%
    slice_max(sentiment, n = 10, with_ties = FALSE) %>%
    kable(caption = "Most negative messages")

# Most negative message
print_message("rec.sport.hockey", 53907)
```

Negative indeed.

### N-gram analysis

Look at using bigrams to counter the effect of negations like 'don't like' and 'not true'.

```{r c9.3.3a, include=TRUE, fig.cap="Words that contributed the most to sentiment when they followed a ‘negating’ word", fig.width=6, fig.height=8, fig.asp=1}
usenet_bigrams <- cleaned_text %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)

usenet_bigram_counts <- usenet_bigrams %>%
    count(newsgroup, bigram, sort = TRUE) %>%
    separate(bigram, c("word1", "word2"), sep = " ")

negate_words <- c("not", "without", "no", "can't", "don't", "won't")

usenet_bigram_counts %>%
    filter(word1 %in% negate_words) %>%
    count(word1, word2, wt = n, sort = TRUE) %>%
    inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
    mutate(contribution = value * n) %>%
    group_by(word1) %>%
    slice_max(abs(contribution), n = 10) %>%
    ungroup() %>%
    mutate(word2 = reorder_within(word2, contribution, word1)) %>%
    ggplot(aes(contribution, word2, fill = contribution > 0)) +
    geom_col(show.legend = FALSE) +
    facet_wrap( ~ word1, scales = "free", nrow = 3) +
    scale_y_reordered() +
    labs(x = "Sentiment value * # of occurrences",
         y = "Words preceded by a negation") +
    theme_light()
```

**Don't** want/like/care.  **No** problem(s).
